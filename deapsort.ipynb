{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Roaming\\Python\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã chọn đối tượng ID: 2\n",
      "Servo X: 89.50, Servo Y: 88.55\n",
      "Servo X: 89.01, Servo Y: 87.10\n",
      "Servo X: 88.48, Servo Y: 85.67\n",
      "Servo X: 87.95, Servo Y: 84.21\n",
      "Servo X: 87.42, Servo Y: 82.79\n",
      "Servo X: 86.96, Servo Y: 81.39\n",
      "Servo X: 86.53, Servo Y: 80.00\n",
      "Servo X: 86.08, Servo Y: 78.58\n",
      "Servo X: 85.66, Servo Y: 77.16\n",
      "Servo X: 84.94, Servo Y: 75.36\n",
      "Servo X: 84.12, Servo Y: 73.42\n",
      "Servo X: 83.26, Servo Y: 71.44\n",
      "Servo X: 82.40, Servo Y: 69.42\n",
      "Servo X: 81.50, Servo Y: 67.38\n",
      "Servo X: 80.63, Servo Y: 65.36\n",
      "Servo X: 79.73, Servo Y: 63.30\n",
      "Servo X: 78.81, Servo Y: 61.19\n",
      "Servo X: 77.85, Servo Y: 59.04\n",
      "Servo X: 76.87, Servo Y: 56.85\n",
      "Servo X: 75.86, Servo Y: 54.61\n",
      "Servo X: 74.83, Servo Y: 52.33\n",
      "Servo X: 73.77, Servo Y: 50.01\n",
      "Servo X: 72.68, Servo Y: 47.65\n",
      "Servo X: 71.56, Servo Y: 45.24\n",
      "Servo X: 70.41, Servo Y: 42.79\n",
      "Servo X: 69.24, Servo Y: 40.30\n",
      "Servo X: 68.04, Servo Y: 37.76\n",
      "Servo X: 66.82, Servo Y: 35.19\n",
      "Servo X: 65.56, Servo Y: 32.57\n",
      "Servo X: 64.28, Servo Y: 29.91\n",
      "Servo X: 62.98, Servo Y: 27.20\n",
      "Servo X: 61.64, Servo Y: 24.45\n",
      "Servo X: 60.28, Servo Y: 21.66\n",
      "Servo X: 58.88, Servo Y: 18.82\n",
      "Servo X: 57.47, Servo Y: 15.94\n",
      "Servo X: 56.02, Servo Y: 13.02\n",
      "Servo X: 54.55, Servo Y: 10.06\n",
      "Servo X: 53.05, Servo Y: 7.06\n",
      "Servo X: 51.52, Servo Y: 4.01\n",
      "Servo X: 49.97, Servo Y: 0.91\n",
      "Servo X: 48.39, Servo Y: 0.00\n",
      "Servo X: 46.78, Servo Y: 0.00\n",
      "Servo X: 45.14, Servo Y: 0.00\n",
      "Servo X: 43.47, Servo Y: 0.00\n",
      "Servo X: 41.78, Servo Y: 0.00\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 96\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     94\u001b[0m detections_for_tracker \u001b[38;5;241m=\u001b[39m [((x, y, x\u001b[38;5;241m+\u001b[39mw, y\u001b[38;5;241m+\u001b[39mh), conf, class_id) \n\u001b[0;32m     95\u001b[0m                           \u001b[38;5;28;01mfor\u001b[39;00m (x, y, w, h, conf, class_id) \u001b[38;5;129;01min\u001b[39;00m detected_objects]\n\u001b[1;32m---> 96\u001b[0m tracks \u001b[38;5;241m=\u001b[39m \u001b[43mtracker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_tracks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdetections_for_tracker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m track \u001b[38;5;129;01min\u001b[39;00m tracks:\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m track\u001b[38;5;241m.\u001b[39mis_confirmed():\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\deep_sort_realtime\\deepsort_tracker.py:199\u001b[0m, in \u001b[0;36mDeepSort.update_tracks\u001b[1;34m(self, raw_detections, embeds, frame, today, others, instance_masks)\u001b[0m\n\u001b[0;32m    196\u001b[0m raw_detections \u001b[38;5;241m=\u001b[39m [d \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m raw_detections \u001b[38;5;28;01mif\u001b[39;00m d[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m d[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 199\u001b[0m     embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_embeds\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw_detections\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstance_masks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minstance_masks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;66;03m# Proper deep sort detection objects that consist of bbox, confidence and embedding.\u001b[39;00m\n\u001b[0;32m    202\u001b[0m detections \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_detections(raw_detections, embeds, instance_masks\u001b[38;5;241m=\u001b[39minstance_masks, others\u001b[38;5;241m=\u001b[39mothers)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\deep_sort_realtime\\deepsort_tracker.py:246\u001b[0m, in \u001b[0;36mDeepSort.generate_embeds\u001b[1;34m(self, frame, raw_dets, instance_masks)\u001b[0m\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedder\u001b[38;5;241m.\u001b[39mpredict(masked_crops)\n\u001b[0;32m    245\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 246\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcrops\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\deep_sort_realtime\\embedder\\embedder_pytorch.py:135\u001b[0m, in \u001b[0;36mMobileNetv2_Embedder.predict\u001b[1;34m(self, np_images)\u001b[0m\n\u001b[0;32m    133\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhalf:\n\u001b[0;32m    134\u001b[0m             this_batch \u001b[38;5;241m=\u001b[39m this_batch\u001b[38;5;241m.\u001b[39mhalf()\n\u001b[1;32m--> 135\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthis_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    137\u001b[0m     all_feats\u001b[38;5;241m.\u001b[39mextend(output\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m all_feats\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\deep_sort_realtime\\embedder\\mobilenetv2_bottle.py:117\u001b[0m, in \u001b[0;36mMobileNetV2_bottle.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m--> 117\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;66;03m# x = self.classifier(x)\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 204\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\deep_sort_realtime\\embedder\\mobilenetv2_bottle.py:61\u001b[0m, in \u001b[0;36mInvertedResidual.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_res_connect:\n\u001b[1;32m---> 61\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     63\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv(x)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 204\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\activation.py:234\u001b[0m, in \u001b[0;36mHardtanh.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhardtanh\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\functional.py:1506\u001b[0m, in \u001b[0;36mhardtanh\u001b[1;34m(input, min_val, max_val, inplace)\u001b[0m\n\u001b[0;32m   1504\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(hardtanh, (\u001b[38;5;28minput\u001b[39m,), \u001b[38;5;28minput\u001b[39m, min_val\u001b[38;5;241m=\u001b[39mmin_val, max_val\u001b[38;5;241m=\u001b[39mmax_val, inplace\u001b[38;5;241m=\u001b[39minplace)\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[1;32m-> 1506\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhardtanh_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_val\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1508\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mhardtanh(\u001b[38;5;28minput\u001b[39m, min_val, max_val)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pyfirmata\n",
    "import threading\n",
    "import time\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "\n",
    "# Cấu hình camera\n",
    "cap = cv2.VideoCapture(0)\n",
    "ws, hs = 1280, 720\n",
    "cap.set(3, ws)\n",
    "cap.set(4, hs)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Camera couldn't Access!!!\")\n",
    "    exit()\n",
    "\n",
    "# Cấu hình Arduino và Servo\n",
    "port = \"COM3\"\n",
    "board = pyfirmata.Arduino(port)\n",
    "servo_x = board.get_pin('d:9:s')\n",
    "servo_y = board.get_pin('d:10:s')\n",
    "\n",
    "servoPos = [90, 90]  # Góc mặc định\n",
    "selected_id = None   # ID của đối tượng được chọn\n",
    "\n",
    "# Khởi tạo DeepSORT\n",
    "tracker = DeepSort(max_age=30, n_init=3, nms_max_overlap=1.0)\n",
    "\n",
    "# Load mô hình YOLO\n",
    "net = cv2.dnn.readNet(\"yolov4-tiny.weights\", \"yolov4-tiny.cfg\")\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "with open(\"coco.names\", \"r\") as f:\n",
    "    classes = f.read().strip().split(\"\\n\")\n",
    "\n",
    "detected_objects = []\n",
    "frame_skip = 3  # Chạy YOLO mỗi 3 frame để giảm tải\n",
    "frame_count = 0\n",
    "\n",
    "def yolo_detection():\n",
    "    global detected_objects, frame_count\n",
    "    while True:\n",
    "        success, img = cap.read()\n",
    "        if not success:\n",
    "            continue\n",
    "\n",
    "        frame_count += 1\n",
    "        if frame_count % frame_skip != 0:\n",
    "            time.sleep(0.01)\n",
    "            continue\n",
    "\n",
    "        blob = cv2.dnn.blobFromImage(img, 0.00392, (416, 416), swapRB=True, crop=False)\n",
    "        net.setInput(blob)\n",
    "        detections = net.forward(output_layers)\n",
    "\n",
    "        detected_objects.clear()\n",
    "        for output in detections:\n",
    "            for detection in output:\n",
    "                scores = detection[5:]\n",
    "                class_id = np.argmax(scores)\n",
    "                confidence = scores[class_id]\n",
    "\n",
    "                if confidence > 0.5 and class_id == 0:\n",
    "                    center_x, center_y, w, h = (detection[:4] * np.array([ws, hs, ws, hs])).astype(int)\n",
    "                    x, y = center_x - w // 2, center_y - h // 2\n",
    "                    detected_objects.append((x, y, w, h, confidence, class_id))\n",
    "        \n",
    "        time.sleep(0.01)\n",
    "\n",
    "def click_event(event, x, y, flags, param):\n",
    "    global selected_id\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        for track in tracker.tracker.tracks:\n",
    "            if not track.is_confirmed():\n",
    "                continue\n",
    "            x1, y1, x2, y2 = track.to_tlbr()\n",
    "            if x1 <= x <= x2 and y1 <= y <= y2:\n",
    "                selected_id = track.track_id\n",
    "                print(f\"Đã chọn đối tượng ID: {selected_id}\")\n",
    "\n",
    "cv2.namedWindow(\"Image\")\n",
    "cv2.setMouseCallback(\"Image\", click_event)\n",
    "\n",
    "yolo_thread = threading.Thread(target=yolo_detection, daemon=True)\n",
    "yolo_thread.start()\n",
    "\n",
    "while True:\n",
    "    success, img = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    detections_for_tracker = [((x, y, x+w, y+h), conf, class_id) \n",
    "                              for (x, y, w, h, conf, class_id) in detected_objects]\n",
    "    tracks = tracker.update_tracks(detections_for_tracker, frame=img)\n",
    "\n",
    "    for track in tracks:\n",
    "        if not track.is_confirmed():\n",
    "            continue\n",
    "\n",
    "        x1, y1, x2, y2 = track.to_tlbr()\n",
    "        track_id = track.track_id\n",
    "\n",
    "        if track_id == selected_id:\n",
    "            obj_center_x = (x1 + x2) // 2\n",
    "            obj_center_y = (y1 + y2) // 2\n",
    "\n",
    "            offset_x = obj_center_x - ws // 2\n",
    "            offset_y = obj_center_y - hs // 2\n",
    "\n",
    "            sensitivity = 0.05\n",
    "            servoPos[0] -= offset_x * sensitivity / (ws // 2) * 45\n",
    "            servoPos[1] -= offset_y * sensitivity / (hs // 2) * 45\n",
    "\n",
    "            servoPos[0] = max(0, min(180, servoPos[0]))\n",
    "            servoPos[1] = max(0, min(180, servoPos[1]))\n",
    "\n",
    "            servo_x.write(servoPos[0])\n",
    "            servo_y.write(servoPos[1])\n",
    "\n",
    "            print(f\"Servo X: {servoPos[0]:.2f}, Servo Y: {servoPos[1]:.2f}\")\n",
    "\n",
    "        color = (0, 255, 0) if track_id == selected_id else (255, 0, 0)\n",
    "        cv2.rectangle(img, (int(x1), int(y1)), (int(x2), int(y2)), color, 3)\n",
    "        cv2.putText(img, f'ID {track_id}', (int(x1), int(y1) - 10),\n",
    "                    cv2.FONT_HERSHEY_PLAIN, 2, color, 2)\n",
    "\n",
    "    cv2.imshow(\"Image\", img)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Roaming\\Python\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import threading\n",
    "import time\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "\n",
    "# Cấu hình camera\n",
    "cap = cv2.VideoCapture(0)\n",
    "ws, hs = 1280, 720\n",
    "cap.set(3, ws)\n",
    "cap.set(4, hs)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Camera couldn't Access!!!\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "servoPos = [90, 90]  # Góc mặc định\n",
    "selected_id = None   # ID của đối tượng được chọn\n",
    "\n",
    "# Khởi tạo DeepSORT\n",
    "tracker = DeepSort(max_age=30, n_init=3, nms_max_overlap=1.0)\n",
    "\n",
    "# Load mô hình YOLO\n",
    "net = cv2.dnn.readNet(\"yolov4-tiny.weights\", \"yolov4-tiny.cfg\")\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "with open(\"coco.names\", \"r\") as f:\n",
    "    classes = f.read().strip().split(\"\\n\")\n",
    "\n",
    "detected_objects = []\n",
    "frame_skip = 3  # Chạy YOLO mỗi 3 frame để giảm tải\n",
    "frame_count = 0\n",
    "\n",
    "def yolo_detection():\n",
    "    global detected_objects, frame_count\n",
    "    while True:\n",
    "        success, img = cap.read()\n",
    "        if not success:\n",
    "            continue\n",
    "\n",
    "        frame_count += 1\n",
    "        if frame_count % frame_skip != 0:\n",
    "            time.sleep(0.01)\n",
    "            continue\n",
    "\n",
    "        blob = cv2.dnn.blobFromImage(img, 0.00392, (416, 416), swapRB=True, crop=False)\n",
    "        net.setInput(blob)\n",
    "        detections = net.forward(output_layers)\n",
    "\n",
    "        detected_objects.clear()\n",
    "        for output in detections:\n",
    "            for detection in output:\n",
    "                scores = detection[5:]\n",
    "                class_id = np.argmax(scores)\n",
    "                confidence = scores[class_id]\n",
    "\n",
    "                if confidence > 0.5 and class_id == 0:\n",
    "                    center_x, center_y, w, h = (detection[:4] * np.array([ws, hs, ws, hs])).astype(int)\n",
    "                    x, y = center_x - w // 2, center_y - h // 2\n",
    "                    detected_objects.append((x, y, w, h, confidence, class_id))\n",
    "        \n",
    "        time.sleep(0.01)\n",
    "\n",
    "def click_event(event, x, y, flags, param):\n",
    "    global selected_id\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        for track in tracker.tracker.tracks:\n",
    "            if not track.is_confirmed():\n",
    "                continue\n",
    "            x1, y1, x2, y2 = track.to_tlbr()\n",
    "            if x1 <= x <= x2 and y1 <= y <= y2:\n",
    "                selected_id = track.track_id\n",
    "                print(f\"Đã chọn đối tượng ID: {selected_id}\")\n",
    "\n",
    "cv2.namedWindow(\"Image\")\n",
    "cv2.setMouseCallback(\"Image\", click_event)\n",
    "\n",
    "yolo_thread = threading.Thread(target=yolo_detection, daemon=True)\n",
    "yolo_thread.start()\n",
    "\n",
    "while True:\n",
    "    success, img = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    detections_for_tracker = [((x, y, x+w, y+h), conf, class_id) \n",
    "                              for (x, y, w, h, conf, class_id) in detected_objects]\n",
    "    tracks = tracker.update_tracks(detections_for_tracker, frame=img)\n",
    "\n",
    "    for track in tracks:\n",
    "        if not track.is_confirmed():\n",
    "            continue\n",
    "\n",
    "        x1, y1, x2, y2 = track.to_tlbr()\n",
    "        track_id = track.track_id\n",
    "\n",
    "        if track_id == selected_id:\n",
    "            obj_center_x = (x1 + x2) // 2\n",
    "            obj_center_y = (y1 + y2) // 2\n",
    "\n",
    "            offset_x = obj_center_x - ws // 2\n",
    "            offset_y = obj_center_y - hs // 2\n",
    "\n",
    "            sensitivity = 0.05\n",
    "            servoPos[0] -= offset_x * sensitivity / (ws // 2) * 45\n",
    "            servoPos[1] -= offset_y * sensitivity / (hs // 2) * 45\n",
    "\n",
    "            servoPos[0] = max(0, min(180, servoPos[0]))\n",
    "            servoPos[1] = max(0, min(180, servoPos[1]))\n",
    "\n",
    "\n",
    "            print(f\"Servo X: {servoPos[0]:.2f}, Servo Y: {servoPos[1]:.2f}\")\n",
    "\n",
    "        color = (0, 255, 0) if track_id == selected_id else (255, 0, 0)\n",
    "        cv2.rectangle(img, (int(x1), int(y1)), (int(x2), int(y2)), color, 3)\n",
    "        cv2.putText(img, f'ID {track_id}', (int(x1), int(y1) - 10),\n",
    "                    cv2.FONT_HERSHEY_PLAIN, 2, color, 2)\n",
    "\n",
    "    cv2.imshow(\"Image\", img)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 391.8ms\n",
      "Speed: 6.7ms preprocess, 391.8ms inference, 10.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 345.1ms\n",
      "Speed: 3.8ms preprocess, 345.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 409.9ms\n",
      "Speed: 3.8ms preprocess, 409.9ms inference, 3.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 458.6ms\n",
      "Speed: 4.7ms preprocess, 458.6ms inference, 3.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 380.4ms\n",
      "Speed: 4.9ms preprocess, 380.4ms inference, 3.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 338.1ms\n",
      "Speed: 5.0ms preprocess, 338.1ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 351.1ms\n",
      "Speed: 2.7ms preprocess, 351.1ms inference, 2.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 375.9ms\n",
      "Speed: 2.7ms preprocess, 375.9ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 328.0ms\n",
      "Speed: 3.5ms preprocess, 328.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 357.9ms\n",
      "Speed: 2.9ms preprocess, 357.9ms inference, 3.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 332.4ms\n",
      "Speed: 2.8ms preprocess, 332.4ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 374.3ms\n",
      "Speed: 7.1ms preprocess, 374.3ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 349.5ms\n",
      "Speed: 3.0ms preprocess, 349.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 346.7ms\n",
      "Speed: 3.2ms preprocess, 346.7ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 352.5ms\n",
      "Speed: 2.9ms preprocess, 352.5ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 357.3ms\n",
      "Speed: 2.6ms preprocess, 357.3ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 352.6ms\n",
      "Speed: 2.8ms preprocess, 352.6ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 381.4ms\n",
      "Speed: 2.7ms preprocess, 381.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Chọn đối tượng ID: 1\n",
      "\n",
      "0: 480x640 1 person, 355.5ms\n",
      "Speed: 2.2ms preprocess, 355.5ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 334.4ms\n",
      "Speed: 2.7ms preprocess, 334.4ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 350.8ms\n",
      "Speed: 2.3ms preprocess, 350.8ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 360.6ms\n",
      "Speed: 3.2ms preprocess, 360.6ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 362.2ms\n",
      "Speed: 4.0ms preprocess, 362.2ms inference, 3.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 350.9ms\n",
      "Speed: 2.7ms preprocess, 350.9ms inference, 3.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 378.5ms\n",
      "Speed: 2.5ms preprocess, 378.5ms inference, 3.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 304.5ms\n",
      "Speed: 3.4ms preprocess, 304.5ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 357.4ms\n",
      "Speed: 3.0ms preprocess, 357.4ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 toilet, 350.5ms\n",
      "Speed: 5.2ms preprocess, 350.5ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 347.7ms\n",
      "Speed: 2.9ms preprocess, 347.7ms inference, 3.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 350.8ms\n",
      "Speed: 2.3ms preprocess, 350.8ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 341.9ms\n",
      "Speed: 2.3ms preprocess, 341.9ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 343.5ms\n",
      "Speed: 2.9ms preprocess, 343.5ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 352.9ms\n",
      "Speed: 2.7ms preprocess, 352.9ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 332.5ms\n",
      "Speed: 2.2ms preprocess, 332.5ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 373.7ms\n",
      "Speed: 2.4ms preprocess, 373.7ms inference, 3.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 375.5ms\n",
      "Speed: 3.0ms preprocess, 375.5ms inference, 3.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 304.5ms\n",
      "Speed: 2.7ms preprocess, 304.5ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 355.1ms\n",
      "Speed: 2.1ms preprocess, 355.1ms inference, 3.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 362.4ms\n",
      "Speed: 3.1ms preprocess, 362.4ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 353.8ms\n",
      "Speed: 2.8ms preprocess, 353.8ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 369.0ms\n",
      "Speed: 2.6ms preprocess, 369.0ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Chọn đối tượng ID: 1\n",
      "Chọn đối tượng ID: 6\n",
      "\n",
      "0: 480x640 1 person, 339.7ms\n",
      "Speed: 3.0ms preprocess, 339.7ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 346.6ms\n",
      "Speed: 2.3ms preprocess, 346.6ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 361.7ms\n",
      "Speed: 2.2ms preprocess, 361.7ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 369.0ms\n",
      "Speed: 2.5ms preprocess, 369.0ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 339.2ms\n",
      "Speed: 2.1ms preprocess, 339.2ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 373.6ms\n",
      "Speed: 3.6ms preprocess, 373.6ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 350.6ms\n",
      "Speed: 3.4ms preprocess, 350.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Chọn đối tượng ID: 1\n",
      "Chọn đối tượng ID: 6\n",
      "\n",
      "0: 480x640 2 persons, 376.2ms\n",
      "Speed: 3.0ms preprocess, 376.2ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 374.1ms\n",
      "Speed: 2.7ms preprocess, 374.1ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 355.7ms\n",
      "Speed: 2.5ms preprocess, 355.7ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 341.9ms\n",
      "Speed: 2.4ms preprocess, 341.9ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Chọn đối tượng ID: 1\n",
      "Chọn đối tượng ID: 6\n",
      "\n",
      "0: 480x640 2 persons, 339.3ms\n",
      "Speed: 2.5ms preprocess, 339.3ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 362.8ms\n",
      "Speed: 3.6ms preprocess, 362.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 390.1ms\n",
      "Speed: 2.9ms preprocess, 390.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 378.2ms\n",
      "Speed: 3.0ms preprocess, 378.2ms inference, 4.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 387.7ms\n",
      "Speed: 2.7ms preprocess, 387.7ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Chọn đối tượng ID: 9\n",
      "Chọn đối tượng ID: 12\n",
      "\n",
      "0: 480x640 2 persons, 368.9ms\n",
      "Speed: 2.7ms preprocess, 368.9ms inference, 5.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 375.0ms\n",
      "Speed: 3.9ms preprocess, 375.0ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 400.8ms\n",
      "Speed: 2.8ms preprocess, 400.8ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 374.7ms\n",
      "Speed: 3.0ms preprocess, 374.7ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 377.4ms\n",
      "Speed: 3.0ms preprocess, 377.4ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 337.4ms\n",
      "Speed: 3.7ms preprocess, 337.4ms inference, 3.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 tie, 325.6ms\n",
      "Speed: 2.6ms preprocess, 325.6ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 tie, 373.3ms\n",
      "Speed: 3.3ms preprocess, 373.3ms inference, 3.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 373.5ms\n",
      "Speed: 2.8ms preprocess, 373.5ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 337.5ms\n",
      "Speed: 2.2ms preprocess, 337.5ms inference, 3.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 tie, 355.5ms\n",
      "Speed: 2.4ms preprocess, 355.5ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 365.9ms\n",
      "Speed: 2.2ms preprocess, 365.9ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 352.6ms\n",
      "Speed: 2.3ms preprocess, 352.6ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 379.2ms\n",
      "Speed: 2.5ms preprocess, 379.2ms inference, 5.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 380.7ms\n",
      "Speed: 2.7ms preprocess, 380.7ms inference, 2.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 383.3ms\n",
      "Speed: 3.0ms preprocess, 383.3ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 369.8ms\n",
      "Speed: 2.7ms preprocess, 369.8ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 346.2ms\n",
      "Speed: 3.6ms preprocess, 346.2ms inference, 2.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 391.8ms\n",
      "Speed: 4.0ms preprocess, 391.8ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 338.8ms\n",
      "Speed: 2.7ms preprocess, 338.8ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 349.4ms\n",
      "Speed: 2.8ms preprocess, 349.4ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 357.5ms\n",
      "Speed: 6.5ms preprocess, 357.5ms inference, 4.6ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pyfirmata\n",
    "from ultralytics import YOLO\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "from filterpy.kalman import KalmanFilter\n",
    "\n",
    "# Kết nối Arduino\n",
    "port = \"COM3\"\n",
    "board = pyfirmata.Arduino(port)\n",
    "servo_pinX = board.get_pin('d:9:s')\n",
    "servo_pinY = board.get_pin('d:10:s')\n",
    "\n",
    "# Load YOLO model\n",
    "model = YOLO(\"model.pt\")\n",
    "\n",
    "# Deep SORT Tracker\n",
    "tracker = DeepSort(max_age=50, max_iou_distance=0.5)\n",
    "\n",
    "# Kalman Filter\n",
    "kf = KalmanFilter(dim_x=4, dim_z=2)\n",
    "kf.F = np.array([[1, 1, 0, 0], [0, 1, 0, 0], [0, 0, 1, 1], [0, 0, 0, 1]])\n",
    "kf.H = np.array([[1, 0, 0, 0], [0, 0, 1, 0]])\n",
    "kf.P *= 1000\n",
    "kf.x = np.array([0, 0, 0, 0])\n",
    "\n",
    "frame_step = 5  #số frame giữa các lần YOLO chạy \n",
    "ws, hs = 1280, 720  # Kích thước khung hình\n",
    "selected_id = None  # ID của đối tượng đang theo dõi\n",
    "\n",
    "# Mở Camera\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "frame_count = 0  # Đếm số frame\n",
    "detections = []  # Lưu kết quả YOLO giữa các lần chạy\n",
    "\n",
    "# Hàm kiểm tra IoU để tránh trùng ID\n",
    "def iou(box1, box2):\n",
    "    x1, y1, x2, y2 = box1\n",
    "    x1_, y1_, x2_, y2_ = box2\n",
    "    xi1, yi1, xi2, yi2 = max(x1, x1_), max(y1, y1_), min(x2, x2_), min(y2, y2_)\n",
    "    inter_area = max(0, xi2 - xi1) * max(0, yi2 - yi1)\n",
    "    box1_area = (x2 - x1) * (y2 - y1)\n",
    "    box2_area = (x2_ - x1_) * (y2_ - y1_)\n",
    "    union_area = box1_area + box2_area - inter_area\n",
    "    return inter_area / union_area if union_area > 0 else 0\n",
    "\n",
    "# Sự kiện click chuột để chọn đối tượng\n",
    "cv2.namedWindow(\"Tracking\")\n",
    "def select_object(event, x, y, flags, param):\n",
    "    global selected_id\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        for track in param:\n",
    "            x1, y1, x2, y2 = map(int, track.to_ltrb())\n",
    "            if x1 <= x <= x2 and y1 <= y <= y2:\n",
    "                selected_id = track.track_id\n",
    "                print(f\"Chọn đối tượng ID: {selected_id}\")\n",
    "\n",
    "cv2.setMouseCallback(\"Tracking\", select_object, param=[])\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "    frame_count += 1\n",
    "\n",
    "    if frame_count % frame_step == 0:\n",
    "        results = model(frame)\n",
    "        new_detections = []\n",
    "        for box in results[0].boxes:\n",
    "            x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "            confidence = box.conf[0].item()\n",
    "            class_id = int(box.cls[0].item())\n",
    "            if class_id == 0:  # Chỉ giữ lại người\n",
    "                new_detections.append(([x1, y1, x2 - x1, y2 - y1], confidence, class_id))\n",
    "        detections = new_detections\n",
    "\n",
    "    # Cập nhật tracker\n",
    "    tracks = tracker.update_tracks(detections, frame=frame)\n",
    "    cv2.setMouseCallback(\"Tracking\", select_object, param=tracks)\n",
    "\n",
    "    for track in tracks:\n",
    "        if not track.is_confirmed():\n",
    "            continue\n",
    "        track_id = track.track_id\n",
    "        x1, y1, x2, y2 = map(int, track.to_ltrb())\n",
    "        cx, cy = (x1 + x2) // 2, (y1 + y2) // 2\n",
    "\n",
    "        if selected_id is not None and track_id == selected_id:\n",
    "            if frame_count % frame_step == 0:\n",
    "                kf.update([cx, cy])\n",
    "            else:\n",
    "                kf.predict()\n",
    "            predicted = kf.x[:2]\n",
    "            servo_x = np.clip(np.interp(predicted[0], [0, ws], [180, 0]), 0, 180)\n",
    "            servo_y = np.clip(np.interp(predicted[1], [0, hs], [180, 0]), 0, 180)\n",
    "            servo_pinX.write(servo_x)\n",
    "            servo_pinY.write(servo_y)\n",
    "\n",
    "        color = (0, 255, 0) if track_id == selected_id else (255, 0, 0)\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "        cv2.putText(frame, f\"ID {track_id}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "    cv2.imshow(\"Tracking\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
